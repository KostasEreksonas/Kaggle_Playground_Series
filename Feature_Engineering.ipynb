{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77117e58-497a-4aa8-816e-14b7d9f146d1",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec12471-032f-4611-8378-5f65f4518ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping\n",
    "from lightgbm import LGBMRegressor, Dataset\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import optuna\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01199ffe-62ec-4d19-a9ce-ff8a06568551",
   "metadata": {},
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca0d5d-b476-46c5-b18f-35283b701e4a",
   "metadata": {},
   "source": [
    "## 2.1 Load train/data datasets\n",
    "\n",
    "Original data was downloaded from [here](https://archive.ics.uci.edu/dataset/1/abalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d7cff-939d-4efc-a961-aef37a893d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import abalone data from Kaggle\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# Import original data\n",
    "original_names = pd.read_csv('data/abalone.names', names=['names'])\n",
    "original = pd.read_csv('data/abalone.data', names=original_names['names'])\n",
    "\n",
    "# Copy train and test datasets\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()\n",
    "\n",
    "# Tag original dataset\n",
    "original[\"original\"] = 1\n",
    "train[\"original\"] = 0\n",
    "test[\"original\"] = 0\n",
    "\n",
    "# Drop 'id' columns\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Rename train and test column names to match names of the original dataset\n",
    "train = train.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\n",
    "test = test.rename(columns={'Whole weight':'Whole_weight','Whole weight.1':'Shucked_weight', 'Whole weight.2':'Viscera_weight', 'Shell weight':'Shell_weight'})\n",
    "\n",
    "# Concatenate train and original datasets\n",
    "train = pd.concat([train, original], axis='rows')\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Set device\n",
    "device='gpu'\n",
    "\n",
    "# Set target\n",
    "target='Rings'\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5020c0-c3eb-4abb-9bc7-c72b5b94b9f2",
   "metadata": {},
   "source": [
    "## 2.2 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f5cb1-009c-49aa-bf88-0331c50cd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = ['Column Name', 'Data Type', 'Train Missing %', 'Test Missing %']\n",
    "for column in train.columns:\n",
    "    data_type = str(train[column].dtype)\n",
    "    non_null_count_train = 100-train[column].count()/train.shape[0]*100\n",
    "    if column != target:\n",
    "        non_null_count_test = 100-test[column].count()/test.shape[0]*100\n",
    "    else:\n",
    "        non_null_count_test = \"NA\"\n",
    "    table.add_row([column, data_type, non_null_count_train,non_null_count_test])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b295a-a284-45f0-b03d-62655248d1d1",
   "metadata": {},
   "source": [
    "# 3. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1f125-b20f-4d70-9517-1bab2fa3b3b1",
   "metadata": {},
   "source": [
    "# 3.1 Target analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28033a-5122-4729-b2b4-c69a817c8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_density(class_0, class_1, title):\n",
    "    # Calculate mean and median of targets\n",
    "    mean_0 = np.mean(class_0)\n",
    "    mean_1 = np.mean(class_1)\n",
    "    median_0 = np.median(class_0)\n",
    "    median_1 = np.median(class_1)\n",
    "\n",
    "    # Draw histogram and density plots\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Create histograms\n",
    "    ax.hist(class_0, bins=20, density=True, alpha=0.5, label='Original=0 Histogram')\n",
    "    ax.hist(class_1, bins=20, density=True, alpha=0.5, label='Original=1 Histogram')\n",
    "\n",
    "    # Get x-axis values for density plot (class 0 and class 1)\n",
    "    x_values_0 = np.linspace(class_0.min(), class_0.max(), len(class_0))\n",
    "    x_values_1 = np.linspace(class_1.min(), class_1.max(), len(class_0))\n",
    "\n",
    "    # Get density values of class 0 and class 1\n",
    "    density_values_0 = (1 / (np.sqrt(2 * np.pi) * np.std(class_0))) * np.exp(-0.5 * ((x_values_0 - mean_0) / np.std(class_0))**2)\n",
    "    density_values_1 = (1 / (np.sqrt(2 * np.pi) * np.std(class_1))) * np.exp(-0.5 * ((x_values_1 - mean_1) / np.std(class_1))**2)\n",
    "\n",
    "    # Draw density plots\n",
    "    ax.plot(x_values_0, density_values_0, color='red', label='Original=0 Density')\n",
    "    ax.plot(x_values_1, density_values_1, color='green', label='Original=1 Density')\n",
    "\n",
    "    # Draw mean plots\n",
    "    ax.axvline(mean_0, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=0)')\n",
    "    ax.axvline(mean_1, color='blue', linestyle='dashed', linewidth=2, label='Mean (Original=1)')\n",
    "\n",
    "    # Draw median plots\n",
    "    ax.axvline(median_0, color='green', linestyle='dashed', linewidth=2, label='Median (Original=0)')\n",
    "    ax.axvline(median_1, color='red', linestyle='dashed', linewidth=2, label='Median (Original=1)')\n",
    "\n",
    "    # Set labels and a title\n",
    "    ax.set_xlabel(target)\n",
    "    ax.set_ylabel('Frequency / Density')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set x-axis view limits\n",
    "    x_min = min(min(class_0), min(class_1))\n",
    "    x_max = max(max(class_0), max(class_1))\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "\n",
    "    # Create a legend\n",
    "    ax.legend(bbox_to_anchor=(1,1),fancybox=False,shadow=False, loc='upper left')\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e996ff-dfb4-4f64-952c-50db1c254986",
   "metadata": {},
   "source": [
    "### 3.1.1 Histogram and density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fea83-cd07-4715-9399-cc7dea36d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target columns for train and original datasets\n",
    "class_0 = train[train['original'] == 0][target]\n",
    "class_1 = train[train['original'] == 1][target]\n",
    "\n",
    "# Draw plot\n",
    "histogram_density(class_0, class_1, 'Histograms and Density Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfff97-8b48-455d-b91d-006dabe8fbb9",
   "metadata": {},
   "source": [
    "### 3.1.2 Log transformed histogram and density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88011e-b45a-4984-a8f3-93cc3e556c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log transformed target columns for train and original datasets\n",
    "class_0 = np.log1p(train[train['original'] == 0][target])\n",
    "class_1 = np.log1p(train[train['original'] == 1][target])\n",
    "\n",
    "# Draw plot\n",
    "histogram_density(class_0, class_1, 'Log Transformed Histograms and Density Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930fe97-b0e0-4445-98e2-4bb875fa38a5",
   "metadata": {},
   "source": [
    "## 3.2 Train & test data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c4299-5cc3-4613-9c65-a28f99b246d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only numeric parameters with more than 2 unique elements (exclude target column)\n",
    "cont_cols=[f for f in train.columns if train[f].dtype in [float,int] and train[f].nunique() > 2 and f not in [target]]\n",
    "\n",
    "# Calculate number of rows needed for the subplots\n",
    "num_rows = len(cont_cols)\n",
    "while num_rows % 3 != 0: num_rows += 1\n",
    "num_rows //= 3\n",
    "\n",
    "# Create subplots for each continuous column\n",
    "fig, axs = plt.subplots(num_rows, 3, figsize=(15, num_rows*5))\n",
    "\n",
    "# Loop through each continuous column and plot the histograms\n",
    "for i, col in enumerate(cont_cols):\n",
    "    # Determine range of values to plot\n",
    "    max_val = max(train[col].max(), test[col].max(), original[col].max())\n",
    "    min_val = min(train[col].min(), test[col].min(), original[col].min())\n",
    "    range_val = max_val - min_val\n",
    "\n",
    "    # Determine the bin size and number of bins\n",
    "    bin_size = range_val / 20\n",
    "    num_bins_train = round(range_val / bin_size)\n",
    "    num_bins_test = round(range_val / bin_size)\n",
    "    num_bins_original = round(range_val / bin_size)\n",
    "\n",
    "    # Calculate the subplot position\n",
    "    row = i // 3\n",
    "    col_pos = i % 3\n",
    "\n",
    "    # Plot the histograms\n",
    "    sns.histplot(train[col], ax=axs[row][col_pos], color='orange', kde=True, label='Train', bins=num_bins_train)\n",
    "    sns.histplot(test[col], ax=axs[row][col_pos], color='green', kde=True, label='Test', bins=num_bins_test)\n",
    "    sns.histplot(original[col], ax=axs[row][col_pos], color='blue', kde=True, label='Original', bins=num_bins_original)\n",
    "    axs[row][col_pos].set_title(col)\n",
    "    axs[row][col_pos].set_xlabel('Value')\n",
    "    axs[row][col_pos].set_ylabel('Frequency')\n",
    "    axs[row][col_pos].legend()\n",
    "\n",
    "# Remove any empty subplots\n",
    "if len(cont_cols) % 3 != 0:\n",
    "    for col_pos in range(len(cont_cols) %3, 3):\n",
    "        axs[-1][col_pos].remove()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb9c06-d632-45c4-8bf6-538a2849bdb6",
   "metadata": {},
   "source": [
    "## 3.3 Sex & Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69a6cd-5a03-464c-825a-4900e1c9c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=original, vars=cont_cols+[target], hue='Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763853a9-b9cf-4520-9847-17e6e918468f",
   "metadata": {},
   "source": [
    "## 3.4 Rings vs Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09564775-64ad-4b2a-957a-d3cbae41408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16, 5))\n",
    "sns.violinplot(x = 'Sex', y = col, data = train, palette = 'pastel')\n",
    "plt.title('Rings Distribution by Sex', fontsize=14)\n",
    "plt.xlabel('Sex', fontsize=12)\n",
    "plt.ylabel('Rings', fontsize=12)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ed0c2-1b7f-4df7-a833-1a113bddabbd",
   "metadata": {},
   "source": [
    "## 3.5. Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1d567-aefc-4b54-ad68-cdb3774c11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of continuous data in test set\n",
    "features = [f for f in test.columns if f != 'Sex']\n",
    "corr = train[features].corr()\n",
    "plt.figure(figsize = (8, 6), dpi = 300)\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr, mask = mask, cmap = 'magma', annot = True, annot_kws = {'size' : 6})\n",
    "plt.title('Features Correlation Matrix\\n', fontsize = 15, weight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef54755-f695-442b-8b2b-98872dc19a9b",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24591d-fbdd-48ee-9a7c-c819f9c30cc0",
   "metadata": {},
   "source": [
    "## 4.1 Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f8555-fdac-4af0-a26a-193f43b4467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    return np.square(np.subtract(y_true, y_pred)).mean()\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.square(np.subtract(y_true, y_pred)).mean())\n",
    "\n",
    "def RMSLE(y_true, y_pred):\n",
    "    return np.sqrt(np.square(np.subtract(np.log(1 + y_pred), np.log(1 + y_true))).mean())\n",
    "\n",
    "def min_max_scaler(train, test, column):\n",
    "    scaler = MinMaxScaler()\n",
    "    max_val = max(train[column].max(), test[column].max())\n",
    "    min_val = min(train[column].min(), test[column].min())\n",
    "    train[column] = (train[column] - min_val) / (max_val - min_val)\n",
    "    test[column] = (test[column] - min_val) / (max_val - min_val)\n",
    "    return train, test\n",
    "\n",
    "#global y_unique\n",
    "#y_unique=train[\"Rings\"].unique()\n",
    "\n",
    "def nearest(y_predicted):\n",
    "    \n",
    "    y_original=y_unique\n",
    "    modified_prediction = np.zeros_like(y_predicted)\n",
    "\n",
    "    for i, y_pred in enumerate(y_predicted):\n",
    "        nearest_value = min(y_original, key=lambda x: abs(x - y_pred))\n",
    "        modified_prediction[i] = nearest_value\n",
    "\n",
    "    return modified_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449e8ba-d954-4b5c-900a-d037d66bd554",
   "metadata": {},
   "source": [
    "## 4.2 New Features\n",
    "\n",
    "1. ***Top Surface Area:*** Length X Diameter\n",
    "2. ***Water Loss:*** During the experiment of data collection, it is possible to have some water loss after dissecting the crab to measure different weights\n",
    "3. ***Abalone Density:*** Measure of body density\n",
    "4. ***BMI:*** Body Mass index\n",
    "5. ***Measurement ratios:*** You can also calculate ratios like length/height, length/diameter.\n",
    "6. ***Incorrect Weights:*** I have noticed that there are sub weight columns greater than the total weight of the Abalone. Part of the body cannot have more weight that the whole body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe73be0-33d6-45f5-aa0f-01d0e388d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(data):\n",
    "    df=data.copy()\n",
    "    \n",
    "    # Clean the weights by capping the over weights with total body weights\n",
    "    df['Shell_weight']=np.where(df['Shell_weight']>df['Whole_weight'],df['Whole_weight'],df['Shell_weight'])\n",
    "    df['Viscera_weight']=np.where(df['Viscera_weight']>df['Whole_weight'],df['Whole_weight'],df['Viscera_weight'])\n",
    "    df['Shucked_weight']=np.where(df['Shucked_weight']>df['Whole_weight'],df['Whole_weight'],df['Shucked_weight'])\n",
    "    \n",
    "    # Abalone Surface area\n",
    "    df[\"surface_area\"]=df[\"Length\"]*df[\"Diameter\"]\n",
    "    df['total_area']=2*(df[\"surface_area\"]+df[\"Height\"]*df[\"Diameter\"]+df[\"Length\"]*df[\"Height\"])\n",
    "    \n",
    "    # Abalone density approx\n",
    "    df['approx_density']=df['Whole_weight']/(df['surface_area']*df['Height']+1e-5)\n",
    "    \n",
    "    # Abalone BMI\n",
    "    df['bmi']=df['Whole_weight']/(df['Height']**2+1e-5)\n",
    "    \n",
    "    # Measurement derived\n",
    "    df[\"length_dia_ratio\"]=df['Length']/(df['Diameter']+1e-5)\n",
    "    df[\"length_height_ratio\"]=df['Length']/(df['Height']+1e-5)\n",
    "    df['shell_shuck_ratio']=df[\"Shell_weight\"]/(df[\"Shucked_weight\"]+1e-5)\n",
    "    df['shell_viscera_ratio']=df['Shell_weight']/(df['Viscera_weight']+1e-5)\n",
    "    \n",
    "    df['viscera_tot_ratio']=df['Viscera_weight']/(df['Whole_weight']  +1e-5)\n",
    "    df['shell_tot_ratio']=df['Shell_weight']/(df['Whole_weight']    +1e-5)\n",
    "    df['shuck_tot_ratio']=df['Shucked_weight']/(df['Whole_weight']   +1e-5)\n",
    "    df['shell_body_ratio']=df['Shell_weight']/(df['Shell_weight']+df['Whole_weight']+1e-5)\n",
    "    df['flesh_ratio']=df['Shucked_weight']/(df['Whole_weight']+df['Shucked_weight']+1e-5)\n",
    "    \n",
    "    df['inv_viscera_tot']= df['Whole_weight'] / (df['Viscera_weight']+1e-5)\n",
    "    df['inv_shell_tot']= df['Whole_weight'] /( df['Shell_weight']+1e-5)\n",
    "    df['inv_shuck_tot']= df['Whole_weight'] / (df['Shucked_weight']+1e-5)\n",
    "    \n",
    "    \n",
    "    # Water Loss during experiment\n",
    "    df[\"water_loss\"]=df[\"Whole_weight\"]-df[\"Shucked_weight\"]-df['Viscera_weight']-df['Shell_weight']\n",
    "    df[\"water_loss\"]=np.where(df[\"water_loss\"]<0,min(df[\"Shucked_weight\"].min(),df[\"Viscera_weight\"].min(),df[\"Shell_weight\"].min()),df[\"water_loss\"])\n",
    "    return df\n",
    "\n",
    "train = new_features(train)\n",
    "test = new_features(test)\n",
    "original = new_features(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad141cc3-4489-4d49-a32e-c52b0583acc3",
   "metadata": {},
   "source": [
    "## 4.3 Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09abce-c023-40af-9d90-958f14134e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels from data\n",
    "y = train[\"Rings\"]\n",
    "train = train.drop(['original', 'Rings'], axis=1)\n",
    "test = test.drop(['original'], axis=1)\n",
    "\n",
    "# Get numerical columns\n",
    "columns = [col for col in train.columns if train[col].dtype not in [\"object\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c100d5-31c9-4356-be1b-1c81919d24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "global unimportant_features\n",
    "global overall_best_score\n",
    "global overall_best_col\n",
    "unimportant_features=[]\n",
    "overall_best_score=1e5\n",
    "overall_best_col='none'\n",
    "def transform_column(train, test, columns, target):\n",
    "    # Make a copy of an original dataframe\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    # Get feature and score variables\n",
    "    global unimportant_features\n",
    "    global overall_best_score\n",
    "    global overall_best_col\n",
    "    # Instantiate a table with field names\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Original RMSLE', 'Transformation', 'Tranformed RMSLE']\n",
    "    # Define parameters\n",
    "    epsilon = 1e-5\n",
    "    power_0_25 = lambda x: np.power(x + 1 - np.min(x), 0.25)\n",
    "    power_2 = lambda x: np.power(x + 1 - np.min(x), 2)\n",
    "    # Define transformers\n",
    "    transformer_box_cox = PowerTransformer(method = 'box-cox')\n",
    "    transformer_yeo_johnson = PowerTransformer(method = 'yeo-johnson')\n",
    "    transformer_power_0_25 = FunctionTransformer(power_0_25)\n",
    "    transformer_power_2 = FunctionTransformer(power_2)\n",
    "    # Append transformed columns to the datafame\n",
    "    for col in columns:\n",
    "        # Scale train and test data\n",
    "        train, test = min_max_scaler(train, test, col)\n",
    "        # Log transformation\n",
    "        train_copy['log_'+col] = np.log1p(train_copy[col])\n",
    "        test_copy['log_'+col] = np.log1p(test_copy[col])\n",
    "        # Square root transformation\n",
    "        train_copy['sqrt_'+col] = np.sqrt(train_copy[col])\n",
    "        test_copy['sqrt_'+col] = np.sqrt(test_copy[col])\n",
    "        # Box Cox transformation\n",
    "        train_copy['box_cox_'+col] = transformer_box_cox.fit_transform(train_copy[[col]] + epsilon)\n",
    "        test_copy['box_cox_'+col] = transformer_box_cox.fit_transform(test_copy[[col]] + epsilon)\n",
    "        # Yeo Johnson transformation\n",
    "        train_copy['yeo_johnson_'+col] = transformer_yeo_johnson.fit_transform(train_copy[[col]])\n",
    "        test_copy['yeo_johnson_'+col] = transformer_yeo_johnson.fit_transform(test_copy[[col]])\n",
    "        # **0.25 transformation\n",
    "        train_copy['power_0_25_'+col] = transformer_power_0_25.fit_transform(train_copy[[col]])\n",
    "        test_copy['power_0_25_'+col] = transformer_power_0_25.fit_transform(test_copy[[col]])\n",
    "        # **2 transformation\n",
    "        train_copy['power_0_2_'+col] = transformer_power_2.fit_transform(train_copy[[col]])\n",
    "        test_copy['power_0_2_'+col] = transformer_power_2.fit_transform(test_copy[[col]])\n",
    "        # Log to power transformation\n",
    "        train_copy['log_sqrt_'+col] = np.log1p(np.sqrt(train_copy[col]))\n",
    "        test_copy['log_sqrt_'+col] = np.log1p(np.sqrt(test_copy[col]))\n",
    "        # Get transformed column names\n",
    "        transformed_columns = [col, \"log_\"+col, \"sqrt_\"+col, \"box_cox_\"+col, \"yeo_johnson_\"+col,  \"power_0_25_\"+col , \"power_0_2_\"+col,\"log_sqrt_\"+col]\n",
    "        # TruncatedSVD\n",
    "        pca = TruncatedSVD(n_components=1)\n",
    "        x_pca_train = pca.fit_transform(train_copy[transformed_columns])\n",
    "        x_pca_test = pca.transform(test_copy[transformed_columns])\n",
    "        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n",
    "        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n",
    "        transformed_columns.append(col+\"_pca_comb\")\n",
    "        \n",
    "        test_copy = test_copy.reset_index(drop=True)\n",
    "        \n",
    "        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n",
    "        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        # Evaluate performance of each data transformation\n",
    "        rmse_scores = []\n",
    "        for t_col in transformed_columns:\n",
    "            # Get array of train and test columns\n",
    "            X = train_copy[[t_col]].values\n",
    "            y = target.values\n",
    "            # Save rmse score for every fold\n",
    "            rmses = []\n",
    "            for train_idx, val_idx in kf.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                x_val, y_val = X[val_idx], y[val_idx]\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, np.log1p(y_train))\n",
    "                y_pred = nearest(np.expm1(model.predict(x_val)))\n",
    "                rmses.append(RMSE(np.log1p(y_val),np.log1p(y_pred)))\n",
    "            rmse_scores.append((t_col, np.mean(rmses)))\n",
    "\n",
    "            if overall_best_score > np.mean(rmses):\n",
    "                overall_best_score = np.mean(rmses)\n",
    "                overall_best_col = t_col\n",
    "            if t_col == col:\n",
    "                orig_rmse = np.mean(rmses)\n",
    "            \n",
    "        best_col, best_rmse = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n",
    "        cols_to_drop = [col for col in transformed_columns if col != best_col]\n",
    "        final_selection = [col for col in transformed_columns if col not in cols_to_drop]\n",
    "        if cols_to_drop:\n",
    "            unimportant_features = unimportant_features+cols_to_drop\n",
    "        table.add_row([col, orig_rmse, best_col, best_rmse])\n",
    "    print(table)\n",
    "    print(f\"Overall best CV RMSLE score: {overall_best_score}\")\n",
    "    return train_copy, test_copy, transformed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88e83f-7bcc-4ccc-863c-7727af31cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_copy, test_copy, transformed_columns = transform_column(train, test, columns, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86add1b4-906a-4d61-a7dd-7d02cfdd3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backup of data\n",
    "train_bak = train\n",
    "test_bak = test\n",
    "\n",
    "# Copy transformed data\n",
    "train = train_copy\n",
    "test = test_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e3dff-5157-45a0-a18f-d18b13786faf",
   "metadata": {},
   "source": [
    "## 4.4 Numerical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d92b7-f330-4448-bdcc-098fe5a3dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Rings\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433128c-b9f3-4f7f-bb4d-ccc861b7b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Cluster WOE Feature', 'RMSLE (CV-TRAIN)']\n",
    "for col in columns:\n",
    "    sub_set=[f for f in unimportant_features if col in f]\n",
    "    #print(sub_set)\n",
    "    temp_train=train[sub_set]\n",
    "    temp_test=test[sub_set]\n",
    "    sc=StandardScaler()\n",
    "    temp_train=sc.fit_transform(temp_train)\n",
    "    temp_test=sc.transform(temp_test)\n",
    "    model = KMeans()\n",
    "\n",
    "    # print(ideal_clusters)\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    kmeans.fit(np.array(temp_train))\n",
    "    labels_train = kmeans.labels_\n",
    "\n",
    "    train[col+\"_unimp_cluster_WOE\"] = labels_train\n",
    "    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n",
    "    \n",
    "    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X=train[[col+\"_unimp_cluster_WOE\"]].values\n",
    "    y=train[target].values\n",
    "\n",
    "    best_rmse=[]\n",
    "    for train_idx, val_idx in kf.split(X,y):\n",
    "        X_train,y_train=X[train_idx],y[train_idx]\n",
    "        x_val,y_val=X[val_idx],y[val_idx]\n",
    "        model=LinearRegression()\n",
    "        model.fit(X_train,np.log1p(y_train))\n",
    "        y_pred=nearest(np.expm1(model.predict(x_val)))\n",
    "        best_rmse.append(RMSE(np.log1p(y_val),np.log1p(y_pred)))\n",
    "        \n",
    "    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(best_rmse)])\n",
    "    if overall_best_score<np.mean(best_rmse):\n",
    "            overall_best_score=np.mean(best_rmse)\n",
    "            overall_best_col=col+\"_unimp_cluster_WOE\"\n",
    "    \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc16bbc-08ab-48ce-a3fe-6be4c1af2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_copy\n",
    "test = test_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a7ce1-3df1-4c55-9875-68af741d6186",
   "metadata": {},
   "source": [
    "## 4.4 Encode categorical data\n",
    "\n",
    "Encode categorical data using `One-Hot` encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04671018-4845-4656-b8cb-5401ceaa3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [f for f in test.columns if (train[f].dtype != 'O' and train[f].nunique()<2000 and train[f].nunique()>2 and \"WOE\" not in f) or (train[f].dtype == 'O') ]\n",
    "#print(train[cat_cols].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cf2ba-9b1b-49d2-a765-162c630de1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train\n",
    "test_bak = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7d1e0-9772-4eb5-bc49-fb6ead9125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak\n",
    "test = test_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc1240-097a-42bc-b0b5-5e6e94bedc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def nearest_val(target):\n",
    "    return min(common, key=lambda x: abs(x - target), default=\"EMPTY\")\n",
    "\n",
    "cat_cols_updated=['Sex']\n",
    "for col in cat_cols:\n",
    "    if train[col].dtype!=\"O\":\n",
    "        train[f\"{col}_cat\"]=train[col]\n",
    "        test[f\"{col}_cat\"]=test[col]\n",
    "        cat_cols_updated.append(f\"{col}_cat\")\n",
    "        uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n",
    "        if uncommon:\n",
    "            common=list(set(test[col].unique())& set(train[col].unique()))\n",
    "            train[f\"{col}_cat\"]=train[col].apply(nearest_val)\n",
    "            test[f\"{col}_cat\"]=test[col].apply(nearest_val)\n",
    "print(train[cat_cols_updated].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fee660-f354-4aaa-a206-598995ed5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns), len(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe06e73-57e3-47fd-8f13-230f43c0ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train\n",
    "test_bak = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400ee55-f8ee-48de-b7b1-e52ce881b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b6f52-c402-4b5a-84ed-9d5ee4398e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak\n",
    "test = test_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14596d-6efd-44d1-86a2-3f9379f189bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace('EMPTY', np.nan)\n",
    "train = train.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc251790-164e-423d-b75c-4b58feea2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.replace('EMPTY', np.nan)\n",
    "test = test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d5f7c-5dc6-448e-88e4-acdde2d6268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [f for f in cat_cols_updated if f in train.columns]\n",
    "cat_cols_updated = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215f9e4-ad35-4e0d-9b15-cc17994dac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0c1c0-096c-42c1-9af7-0bbfd9415294",
   "metadata": {},
   "outputs": [],
   "source": [
    "global overall_best_score\n",
    "# overall_best_score = 0\n",
    "def OHE(train_df,test_df,cols,target):\n",
    "    '''\n",
    "    Function for one hot encoding, it first combines the data so that no category is missed and\n",
    "    the category with least frequency can be dropped because of redundancy\n",
    "    '''\n",
    "    combined = pd.concat([train_df, test_df], axis=0)\n",
    "    for col in cols:\n",
    "        one_hot = pd.get_dummies(combined[col])\n",
    "        counts = combined[col].value_counts()\n",
    "        min_count_category = counts.idxmin()\n",
    "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
    "        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n",
    "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
    "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    \n",
    "    # split back to train and test dataframes\n",
    "    train_ohe = combined[:len(train_df)]\n",
    "    test_ohe = combined[len(train_df):]\n",
    "    test_ohe.reset_index(inplace=True,drop=True)\n",
    "    test_ohe.drop(columns=[target],inplace=True)\n",
    "    return train_ohe, test_ohe\n",
    "\n",
    "def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n",
    "    '''\n",
    "    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n",
    "    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n",
    "    '''\n",
    "    train_copy=train.copy()\n",
    "    test_copy=test.copy()\n",
    "    ohe_cols=[]\n",
    "    for col in extra_cols:\n",
    "        dict1=train_copy[col].value_counts().to_dict()\n",
    "        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
    "        rare_keys=list([*ordered.keys()][n_limit:])\n",
    "#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n",
    "        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n",
    "        \n",
    "        train_copy[col]=train_copy[col].replace(rare_key_map)\n",
    "        test_copy[col]=test_copy[col].replace(rare_key_map)\n",
    "    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n",
    "    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n",
    "    train_copy=train_copy.drop(columns=drop_cols)\n",
    "    test_copy=test_copy.drop(columns=drop_cols)\n",
    "    \n",
    "    return train_copy, test_copy\n",
    "\n",
    "def cat_encoding(train, test,cat_cols_updated, target):\n",
    "    global overall_best_score\n",
    "    global overall_best_col\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Encoded Features', 'RMSLE Score']\n",
    "    train_copy=train.copy()\n",
    "    test_copy=test.copy()\n",
    "    train_dum = train.copy()\n",
    "    for feature in cat_cols_updated:\n",
    "#         print(feature)\n",
    "#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n",
    "#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n",
    "#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n",
    "#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n",
    "\n",
    "        dic = train[feature].value_counts().to_dict()\n",
    "        train_copy[feature + \"_count\"] =train[feature].map(dic)\n",
    "        test_copy[feature + \"_count\"] = test[feature].map(dic)\n",
    "\n",
    "        dic2=train[feature].value_counts().to_dict()\n",
    "#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n",
    "        list1=np.arange(len(dic2.values())) # Higher rank for low count\n",
    "        dic3=dict(zip(list(dic2.keys()),list1))\n",
    "        \n",
    "        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n",
    "        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n",
    "\n",
    "        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#,feature + \"_target\"\n",
    "        #print(train_copy['box_cox_Length_cat'].astype(str))\n",
    "        train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
    "        test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
    "        \n",
    "        if train_copy[feature].nunique()<=15:\n",
    "            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
    "            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
    "            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
    "            \n",
    "        else:\n",
    "            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=15)\n",
    "            \n",
    "        train_copy=train_copy.drop(columns=[feature])\n",
    "        #test_copy=test_copy.drop(columns=[feature])\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        rmse_scores = []\n",
    "\n",
    "        for f in temp_cols:\n",
    "            X = train_copy[[f]].values\n",
    "            y = train_copy[target].astype(int).values\n",
    "\n",
    "            rmses = []\n",
    "            for train_idx, val_idx in kf.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                x_val, y_val = X[val_idx], y[val_idx]\n",
    "                model=LinearRegression()\n",
    "                model.fit(X_train,np.log1p(y_train))\n",
    "                y_pred=nearest(np.expm1(model.predict(x_val)))\n",
    "                rmses.append(RMSE(np.log1p(y_val),np.log1p(y_pred)))\n",
    "            rmse_scores.append((f,np.mean(rmses)))\n",
    "            if overall_best_score > np.mean(rmses):\n",
    "                overall_best_score = np.mean(rmses)\n",
    "                overall_best_col = f\n",
    "        best_col, best_auc = sorted(rmse_scores, key=lambda x: x[1], reverse=False)[0]\n",
    "\n",
    "        corr = train_copy[temp_cols].corr(method='pearson')\n",
    "        corr_with_best_col = corr[best_col]\n",
    "        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n",
    "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
    "        if cols_to_drop:\n",
    "            train_copy = train_copy.drop(columns=cols_to_drop)\n",
    "            test_copy = test_copy.drop(columns=cols_to_drop)\n",
    "\n",
    "        table.add_row([feature, best_col, best_auc])\n",
    "        #print(feature)\n",
    "    print(table)\n",
    "    print(\"overall best CV score: \", overall_best_score)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "train, test= cat_encoding(train, test,cat_cols_updated, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de5e10-391f-4ba0-9b1c-d3d785ec80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns), len(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d11c61-4025-4b8b-aa50-c34f3b9b9c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train\n",
    "test_bak = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c5df3-6c5f-40c3-aa8e-80313d9111f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak\n",
    "test = test_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c90f5-2786-416d-9b4e-1bc81ac7863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = [col for col in train.columns if train[col].dtype == 'O']\n",
    "obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc271429-fd6d-4580-b978-256e89dd1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(columns=obj_cols)\n",
    "test=test.drop(columns=obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42f00b-6a44-4e00-9fd0-f1d55cbcc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_drop=[ f for f in unimportant_features if f in train.columns]\n",
    "train=train.drop(columns=first_drop)\n",
    "test=test.drop(columns=first_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a03fd-7d48-4f54-8cb1-46c48d5cc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_drop_list=[]\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Original', 'Final Transformation', \"RMSLE(CV)- Regression\"]\n",
    "dt_params={'criterion': 'absolute_error'}\n",
    "threshold=0.85\n",
    "# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\n",
    "best_cols=[]\n",
    "\n",
    "for col in cat_cols:\n",
    "    sub_set=[f for f in train.columns if col in f and train[f].nunique()>100]\n",
    "    print(sub_set)\n",
    "    if len(sub_set)>2:\n",
    "        correlated_features = []\n",
    "\n",
    "        for i, feature in enumerate(sub_set):\n",
    "            # Check correlation with all remaining features\n",
    "            for j in range(i+1, len(sub_set)):\n",
    "                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n",
    "                # If correlation is greater than threshold, add to list of highly correlated features\n",
    "                if correlation > threshold:\n",
    "                    correlated_features.append(sub_set[j])\n",
    "\n",
    "        # Remove duplicate features from the list\n",
    "        correlated_features = list(set(correlated_features))\n",
    "        print(correlated_features)\n",
    "        if len(correlated_features)>=2:\n",
    "            print(len(correlated_features))\n",
    "            temp_train=train[correlated_features]\n",
    "            temp_test=test[correlated_features]\n",
    "            #Scale before applying PCA\n",
    "            sc=StandardScaler()\n",
    "            temp_train=sc.fit_transform(temp_train)\n",
    "            temp_test=sc.transform(temp_test)\n",
    "\n",
    "            # Initiate PCA\n",
    "            pca=TruncatedSVD(n_components=1)\n",
    "            x_pca_train=pca.fit_transform(temp_train)\n",
    "            x_pca_test=pca.transform(temp_test)\n",
    "            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n",
    "            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n",
    "            train=pd.concat([train,x_pca_train],axis='columns')\n",
    "            test=pd.concat([test,x_pca_test],axis='columns')\n",
    "\n",
    "            # Clustering\n",
    "            model = KMeans()\n",
    "            kmeans = KMeans(n_clusters=28)\n",
    "            kmeans.fit(np.array(temp_train))\n",
    "            labels_train = kmeans.labels_\n",
    "\n",
    "            train[col+'_final_cluster'] = labels_train\n",
    "            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n",
    "\n",
    "            cat_labels=cat_labels=train.groupby([col+\"_final_cluster\"])[target].mean()\n",
    "            cat_labels2=cat_labels.to_dict()\n",
    "            train[col+\"_final_cluster\"]=train[col+\"_final_cluster\"].map(cat_labels2)\n",
    "            test[col+\"_final_cluster\"]=test[col+\"_final_cluster\"].map(cat_labels2)\n",
    "\n",
    "            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n",
    "\n",
    "            # See which transformation along with the original is giving you the best univariate fit with target\n",
    "            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            rmse_scores = []\n",
    "\n",
    "            for f in transformed_columns:\n",
    "                X = train_copy[[f]].values\n",
    "                y = train_copy[target].astype(int).values\n",
    "\n",
    "                rmses = []\n",
    "                for train_idx, val_idx in kf.split(X, y):\n",
    "                    X_train, y_train = X[train_idx], y[train_idx]\n",
    "                    x_val, y_val = X[val_idx], y[val_idx]\n",
    "                    model=LinearRegression()\n",
    "                    model.fit(X_train,np.log1p(y_train))\n",
    "                    y_pred=nearest(np.expm1(model.predict(x_val)))\n",
    "                    rmses.append(RMSE(np.log1p(y_val),np.log1p(y_pred)))\n",
    "                    \n",
    "                if f not in best_cols:\n",
    "                    rmse_scores.append((f,np.mean(rmses)))\n",
    "            if len(rmse_scores) > 0:\n",
    "                best_col, best_rmse=sorted(rmse_scores, key=lambda x:x[1], reverse=False)[0]\n",
    "                #print(f\"{best_col}, {best_rmse}\")\n",
    "                best_cols.append(best_col)\n",
    "\n",
    "                cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n",
    "                if cols_to_drop:\n",
    "                    final_drop_list=final_drop_list+cols_to_drop\n",
    "                table.add_row([col, best_col, best_rmse])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aaaeaf-16a3-4f2b-b950-f0e590dc8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206ac4a-2acf-4b18-9ab9-b38d2ee2fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2862ea6-33b8-4875-a5dd-ab4580dd9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [f for f in train.columns if f not in final_drop_list]\n",
    "tmp = [f for f in final_features if f not in \"Rings\"]\n",
    "final_features = tmp\n",
    "len(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101fc22-1c55-473f-aac4-127ec60b8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[final_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cb906-3ca7-4fa4-b5dc-13afc9b4d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_scaled=train.copy()\n",
    "#test_scaled=test.copy()\n",
    "\n",
    "train = train[final_features]\n",
    "test = test[final_features]\n",
    "\n",
    "train_scaled = train\n",
    "test_scaled = test\n",
    "\n",
    "train_scaled=sc.fit_transform(train)\n",
    "test_scaled=sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0b981-86a2-4054-887b-d31ce24080e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_scaled\n",
    "test = test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ea1cf-ce1e-4898-8509-08ef0359dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc4c0e-cf5b-44d7-8388-fbbd3d8602c9",
   "metadata": {},
   "source": [
    "## 4.5 Save data\n",
    "\n",
    "Save prepared data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7d2d0-b3c3-4e7d-bcfc-278324a9405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/prepared_final_train.npy', train)\n",
    "np.save('data/prepared_final_test.npy', test)\n",
    "np.save('data/targets.npy', y)\n",
    "#train.save('data/prepared_final_train.npy')\n",
    "#test.save('data/prepared_final_test.npy')\n",
    "#y.save('data/targets.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491d694-472b-4db3-88ae-746f4e564eb7",
   "metadata": {},
   "source": [
    "# 5. Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf26b00-696f-442c-94b5-cb82c89170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport data\n",
    "train = np.load('data/prepared_final_train.npy')\n",
    "test = np.load('data/prepared_final_test.npy')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "y = np.load('data/targets.npy')\n",
    "\n",
    "# Convert target to Pandas Series\n",
    "#y = y.to_dict('dict')['Rings']\n",
    "#y = pd.Series(data=y.values(), index=y.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f20a3-2b22-4a87-b095-4095720fff1d",
   "metadata": {},
   "source": [
    "## 5.1 XGBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5651d-1df5-4c41-9028-4903e0434983",
   "metadata": {},
   "source": [
    "### 5.1.1 Automatic hyperparameter tuning\n",
    "\n",
    "Hyperparameter tuning with `optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3bc3b-333b-4f98-a188-262b67638e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def objective(trial, data=train, target=y, average=0):\n",
    "    param = {\n",
    "        'n_jobs': trial.suggest_categorical('n_jobs', [-1]),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [400]),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise']),\n",
    "        'eval_metric': trial.suggest_categorical('eval_metric', ['rmsle']),\n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror']),\n",
    "        'device' : trial.suggest_categorical('device', ['cuda']),\n",
    "        'tree_method': trial.suggest_categorical('tree_method', ['hist']),  # this parameter means using the GPU when training our model to speedup the training process\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'eta': trial.suggest_float('subsample', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'max_bin': trial.suggest_int('max_bin', 255, 8192),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('lambda', 1.0, 100.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bylevel': trial.suggest_categorical('colsample_bylevel', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'random_state': trial.suggest_categorical('random_state', [42]),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "    }\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "    \n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(data, target):\n",
    "        X_train, y_train = data[train_idx], target[train_idx]\n",
    "        x_val, y_val = data[val_idx], target[val_idx]\n",
    "        xgbr_model = TransformedTargetRegressor(xgb.XGBRegressor(**param),\n",
    "                                                func=np.log1p,\n",
    "                                                inverse_func=np.expm1)\n",
    "        xgbr_model.fit(X_train, y_train, eval_set=[(x_val,y_val)],verbose=0,callbacks=[EarlyStopping(rounds = 300,save_best=True)])\n",
    "        y_preds = xgbr_model.predict(x_val)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        c += 1\n",
    "    gc.collect()\n",
    "    return average/(c-1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d991c2-dea9-48f9-b5cf-1ca4eb78f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial parameters:', study.best_trial.params)\n",
    "print('Mean RMSLE value:', study.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18882a4e-bf5a-481f-8652-6b3134cade55",
   "metadata": {},
   "source": [
    "### 5.1.2 Manual hyperparameter tuning\n",
    "\n",
    "Manual fine-tuning of parameters set by `optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5241060-b07a-4c66-ab33-15dd78d30b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_params = study.best_trial.params\n",
    "xgbr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955fbce-8c24-4adc-a119-16f6db48eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_params = {\n",
    " 'n_jobs': -1,\n",
    " 'n_estimators': 3000,\n",
    " 'grow_policy': 'depthwise',\n",
    " 'eval_metric': 'rmsle',\n",
    " 'objective': 'reg:squarederror',\n",
    " 'device': 'cpu',\n",
    " 'tree_method': 'hist',\n",
    " 'learning_rate': 0.014737232233402146,\n",
    " 'gamma': 0.1706310268870559,\n",
    " 'subsample': 0.3292036531972937,\n",
    " 'max_depth': 10,\n",
    " 'max_bin': 65535,\n",
    " 'colsample_bytree': 0.6974727367761323,\n",
    " 'lambda': 5.734442900265066,\n",
    " 'alpha': 0.022404998725219973,\n",
    " #'colsample_bylevel': 0.4,\n",
    " 'random_state': 42,\n",
    " 'min_child_weight': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8af3a4-bccb-4fec-b28a-2e7fab09570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def xgbr_cross_val(train, y, test, params, average=0):\n",
    "    \"\"\"\n",
    "    Do a K-Fold data cross-validation on a trained XGBRegressor model\n",
    "    \"\"\"\n",
    "    # Make a copy of an original training data set\n",
    "    train_fold = train.copy()\n",
    "    y_fold = y.copy()\n",
    "\n",
    "    # Instantiate a numpy array filled with zeroes\n",
    "    results = np.zeros(len(test), dtype=np.float32)\n",
    "    \n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Prepare data for cross-valdiation split\n",
    "    X = train_fold\n",
    "    y = y_fold\n",
    "\n",
    "    # Instantiate PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Fold', 'Train RMSLE', 'Test RMSLE', 'Average RMSLE']\n",
    "    \n",
    "    # Define best score (set to a high value as lower the score, the better)\n",
    "    best_score = 1e5\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        print(f\"{c} fold\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        x_val, y_val = X[val_idx], y[val_idx]\n",
    "        xgbr_model = xgb.XGBRegressor(**params)\n",
    "        xgbr_model.fit(X_train, y_train, eval_set=[(x_val,y_val)],verbose=2,callbacks=[EarlyStopping(rounds = 500,save_best=True)])\n",
    "        y_preds = xgbr_model.predict(x_val)\n",
    "        predictions = xgbr_model.predict(test)\n",
    "        results = np.add(results, predictions)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        table.add_row([c, xgbr_model.best_score, rmsle, average/c])\n",
    "        if xgbr_model.best_score < best_score:\n",
    "            best_score = rmsle\n",
    "        c += 1\n",
    "\n",
    "    # Print table\n",
    "    print(table)\n",
    "    \n",
    "    # Print RMSLE stats\n",
    "    print(f\"Best RMSLE: {best_score}, mean RMSLE: {average/(c-1)}\")\n",
    "\n",
    "    # Collect garbage\n",
    "    gc.collect()\n",
    "    return best_score, results/(c-1)\n",
    "\n",
    "best_score, predictions = xgbr_cross_val(train, y, test, xgbr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5a982-cdeb-4b71-ad1e-29b0b069d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02f414-a810-4a7f-bb58-02babebcd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate a model\n",
    "xgbr_model = xgb.XGBRegressor(**xgbr_params)\n",
    "\n",
    "# Fit the model with training data\n",
    "xgbr_model.fit(X_train, y_train, eval_set=[(X_test,y_test)],verbose=2,callbacks=[EarlyStopping(rounds = 300,save_best=True)])\n",
    "predictions = xgbr_model.predict(test)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9491c0-24d9-490c-9456-2b59a8e2ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "submission[\"Rings\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a3353-50b1-4c3b-b211-7477c081e545",
   "metadata": {},
   "source": [
    "## 5.2 CatBoost Regression\n",
    "\n",
    "Tune hyperparameters and cross-validate Catboost regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979084c-cc76-44cd-986a-176eb9c776e2",
   "metadata": {},
   "source": [
    "### 5.2.1 Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fbca4-3414-4ff4-a985-697b74edb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def objective(trial, data=train, target=y, average=0):\n",
    "    param = {\n",
    "        'iterations': trial.suggest_categorical('iterations', [300]),\n",
    "        \"verbose\": trial.suggest_categorical('verbose', [False]),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Plain']),\n",
    "        'depth': trial.suggest_int('depth', 6, 10),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 2048),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 100),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 2, 64),\n",
    "        'eval_metric': trial.suggest_categorical('eval_metric', ['Quantile']),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['RMSE']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bernoulli']),\n",
    "        \"grow_policy\": trial.suggest_categorical('grow_policy', ['Lossguide']),\n",
    "        \"task_type\": trial.suggest_categorical('task_type', [\"GPU\"]),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0),\n",
    "        #'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),\n",
    "        \"random_state\": trial.suggest_categorical('random_state', [42]),\n",
    "        \"early_stopping_rounds\": trial.suggest_categorical('early_stopping_rounds', [300])\n",
    "    }\n",
    "    \n",
    "    # Count folds\n",
    "    c = 1\n",
    "    \n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(data, target):\n",
    "        print(f\"Fold {c}\")\n",
    "        X_train, y_train = data[train_idx], target[train_idx]\n",
    "        x_val, y_val = data[val_idx], target[val_idx]\n",
    "        # Get training and testing data\n",
    "        train = Pool(data=X_train, label=y_train)\n",
    "        evaluate = Pool(data=x_val, label=y_val)\n",
    "        # Instantiate the model\n",
    "        model = CatBoostRegressor(**param)\n",
    "        # Fit the model\n",
    "        model.fit(train, use_best_model=True, eval_set=evaluate)\n",
    "        y_preds = model.predict(x_val)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        c += 1\n",
    "    gc.collect()\n",
    "    return average/(c-1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial parameters:', study.best_trial.params)\n",
    "print('Best trial value:', study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7660bb-4196-43d5-8b59-3104f3d64d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = study.best_trial.params\n",
    "cat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8fab4-01e8-4cf9-9f38-46e40b4fc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    " 'iterations': 267,\n",
    " 'verbose': True,\n",
    " 'depth': 10,\n",
    " 'max_bin': 65535,\n",
    " 'l2_leaf_reg': 31.80480657168747,\n",
    " 'min_data_in_leaf': 20,\n",
    " 'random_strength': 2.32234712463821,\n",
    " 'learning_rate': 0.061882021628170504,\n",
    " 'max_leaves': 57,\n",
    " 'eval_metric': 'Quantile',\n",
    " 'loss_function': 'RMSE',\n",
    " 'bootstrap_type': 'Bernoulli',\n",
    " 'grow_policy': 'Lossguide',\n",
    " 'task_type': 'GPU',\n",
    " 'subsample': 0.8982882178328477,\n",
    " 'random_state': 42,\n",
    " 'early_stopping_rounds': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee1d98-8005-4f4a-beb3-1785f2beccd8",
   "metadata": {},
   "source": [
    "### 5.2.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57552b71-2657-4b68-bf7a-24fbf4adcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def cat_cross_val(train, y, test, param, average=0):\n",
    "    \"\"\"\n",
    "    Do a K-Fold data cross-validation on a trained CatBoost Regressor model\n",
    "    \"\"\"\n",
    "    # Make a copy of an original training data set\n",
    "    X = train.copy()\n",
    "    y = y.copy()\n",
    "    \n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Instantiate PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Fold', 'Learn RMSE', 'Validation RMSE', 'Validation RMSLE', 'Mean RMSLE']\n",
    "    \n",
    "    # Define best score (set to a high value as lower the score, the better)\n",
    "    best_score = 1e5\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        print(f\"{c} fold\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        x_val, y_val = X[val_idx], y[val_idx]\n",
    "        # Get training and testing data\n",
    "        train = Pool(data=X_train, label=y_train)\n",
    "        evaluate = Pool(data=x_val, label=y_val)\n",
    "        cat_model = CatBoostRegressor(**param)\n",
    "        cat_model.fit(train, use_best_model=True, eval_set=evaluate)\n",
    "        y_preds = cat_model.predict(x_val)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        table.add_row([c, cat_model.get_best_score()['learn']['RMSE'], cat_model.get_best_score()['validation']['RMSE'], rmsle, average/c])\n",
    "        c += 1\n",
    "    print(table)\n",
    "    print(f\"Best RMSLE score: {best_score}, mean RMSLE score: {average/(c-1)}\")\n",
    "    return best_score\n",
    "\n",
    "best_score = cat_cross_val(train, y, test, cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966ae8c-a477-4042-8b6c-937b9ee51fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.15)\n",
    "X_val = X_test\n",
    "y_val = y_test\n",
    "# Pool data\n",
    "train = Pool(data=X_train, label=y_train)\n",
    "evaluate = Pool(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd9016-15e1-4c87-a93e-7825a81afbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate the model\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "\n",
    "# Fit the model\n",
    "cat_model.fit(train, use_best_model=True, eval_set=evaluate)\n",
    "\n",
    "# Evaluate the model\n",
    "y_preds = cat_model.predict(X_val)\n",
    "rmsle = RMSLE(y_val, y_preds)\n",
    "print(f\"RMSLE: {rmsle}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = cat_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb8416-56bf-4f03-b249-44048ba0b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "submission[\"Rings\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d3f8b-29cb-47f6-9412-1213de483fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best RMSLE score with {cat_params['iterations']} iterations is: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e9e28-9ccd-4b25-835e-993bab7fce8b",
   "metadata": {},
   "source": [
    "## 5.3 LightGBM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce00a4-61ec-41f3-91fd-536602aedd0b",
   "metadata": {},
   "source": [
    "### 5.3.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fb45e-94ff-48ab-a521-07b94a31ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def objective(trial, data=train, target=y, average=0):\n",
    "    param = {\n",
    "        'num_iterations': trial.suggest_categorical('num_iterations', [300]),\n",
    "        'device': trial.suggest_categorical('device', ['gpu']),\n",
    "        'n_jobs': trial.suggest_categorical('n_jobs', [-1]),\n",
    "        'verbose': trial.suggest_categorical('verbose', [-1]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 10),\n",
    "        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n",
    "        'subsample_freq': trial.suggest_int(\"subsample_freq\", 1, 7),\n",
    "        'random_state': trial.suggest_categorical('random_state', [42]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1., 50.),\n",
    "    }\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "    \n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(data, target):\n",
    "        print(f\"Fold {c}\")\n",
    "        X_train, y_train = data[train_idx], target[train_idx]\n",
    "        x_val, y_val = data[val_idx], target[val_idx]\n",
    "        lgbm_model = LGBMRegressor(**param)\n",
    "        lgbm_model.fit(X_train, y_train, eval_set=[(x_val,y_val)],eval_names=[\"valid\"],eval_metric=['MSLE'])\n",
    "        y_preds = lgbm_model.predict(x_val)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        c += 1\n",
    "    return average/(c-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423479a2-04a8-404c-83be-1669e5c60982",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial parameters:', study.best_trial.params)\n",
    "print('Best trial value:', study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a96da5-d5b7-44c2-87c7-ab14bc3116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = study.best_trial.params\n",
    "lgbm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a189c5-62fb-421c-a99b-75d083856843",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    " 'num_iterations': 855,\n",
    " #'early_stopping': 300,\n",
    " 'device': 'gpu',\n",
    " 'n_jobs': -1,\n",
    " 'verbose': 1,\n",
    " 'max_depth': 7,\n",
    " 'num_leaves': 239,\n",
    " 'subsample_freq': 7,\n",
    " 'random_state': 42,\n",
    " 'min_child_samples': 176,\n",
    " 'reg_alpha': 0.013992152157392416,\n",
    " 'reg_lambda': 2.515505626893547,\n",
    " 'subsample': 0.9369891807768302,\n",
    " 'learning_rate': 0.029000539629072005,\n",
    " 'colsample_bytree': 0.4406989114710534,\n",
    " 'min_child_weight': 39.887991868149506\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1cb6be-a6b0-4c82-b9bf-d1d71c0b09fd",
   "metadata": {},
   "source": [
    "### 5.3.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc77eab-1400-4347-93f7-6dcdc45a4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def lgbm_cross_val(train, y, test, param, average=0):\n",
    "    \"\"\"\n",
    "    Do a K-Fold data cross-validation on a trained LGBM Regression model\n",
    "    \"\"\"\n",
    "    # Make a copy of an original training data set\n",
    "    train_fold = train.copy()\n",
    "    y_fold = y.copy()\n",
    "    \n",
    "    # Instantiate a numpy array filled with zeroes\n",
    "    results = np.zeros(len(test), dtype=np.float32)\n",
    "\n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Prepare data for cross-valdiation split\n",
    "    X = train_fold\n",
    "    y = y_fold\n",
    "\n",
    "    # Instantiate PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Fold', 'Validation RMSE', 'Validation RMSLE', 'Mean RMSLE']\n",
    "    \n",
    "    # Define best score (set to a high value as lower the score, the better)\n",
    "    best_score = 1e5\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        print(f\"{c} fold\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        x_val, y_val = X[val_idx], y[val_idx]\n",
    "        lgbm_model = LGBMRegressor(**param)\n",
    "        lgbm_model.fit(X_train, y_train, eval_set=[(x_val,y_val)],eval_names=[\"valid\"],eval_metric=['MSLE'])\n",
    "        y_preds = lgbm_model.predict(x_val)\n",
    "        predictions = lgbm_model.predict(test)\n",
    "        results = np.add(results, predictions)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        table.add_row([c, lgbm_model.best_score_['valid']['l2'], rmsle, average/c])\n",
    "        if best_score > rmsle:\n",
    "            best_score = rmsle\n",
    "        c += 1\n",
    "    print(table)\n",
    "    print(f\"Best RMSLE score: {best_score}\")\n",
    "    return best_score, results/(c-1)\n",
    "\n",
    "best_score, predictions = lgbm_cross_val(train, y, test, lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda92cc-7fb8-4476-8fcb-f3ebf235b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "lgbm_model.fit(X_train, y_train, eval_set=[(X_test,y_test)],eval_names=[\"valid\"],eval_metric=['MSLE'])\n",
    "y_preds = lgbm_model.predict(x_val)\n",
    "rmsle = RMSLE(y_val, y_preds)\n",
    "print(f\"RMSLE: {rmsle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d204b9f-8c12-4bf9-82b5-2155f37fa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "submission[\"Rings\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cf2fa-1600-4b00-8496-1a7666b306df",
   "metadata": {},
   "source": [
    "## 5.4 Voting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addebac-4d97-4eee-86f6-e818c9f45e42",
   "metadata": {},
   "source": [
    "### 5.4.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e204cb4-2bda-4d38-8501-9b0e3f8a3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_params = {\n",
    " 'n_jobs': -1,\n",
    " 'n_estimators': 3000,\n",
    " 'grow_policy': 'depthwise',\n",
    " 'eval_metric': 'rmsle',\n",
    " 'objective': 'reg:squarederror',\n",
    " 'device': 'cuda',\n",
    " 'tree_method': 'hist',\n",
    " 'learning_rate': 0.04404749587959873,\n",
    " 'gamma': 0.10364742524805404,\n",
    " 'subsample': 0.9376708896501253,\n",
    " 'max_depth': 9,\n",
    " 'max_bin': 1655,\n",
    " 'colsample_bytree': 0.7880464684349316,\n",
    " 'lambda': 8.31564500887863,\n",
    " 'alpha': 2.083946759558932,\n",
    " 'colsample_bylevel': 1.0,\n",
    " 'random_state': 42,\n",
    " 'min_child_weight': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20c675-f22b-4868-9c28-83bb645d9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    " 'iterations': 3000,\n",
    " 'verbose': True,\n",
    " 'depth': 10,\n",
    " 'max_bin': 1419,\n",
    " 'l2_leaf_reg': 4.300237034450856,\n",
    " 'min_data_in_leaf': 86,\n",
    " 'random_strength': 2.0251226145225476,\n",
    " 'learning_rate': 0.05582135516107398,\n",
    " 'max_leaves': 63,\n",
    " 'eval_metric': 'Quantile',\n",
    " 'loss_function': 'RMSE',\n",
    " 'bootstrap_type': 'Bernoulli',\n",
    " 'grow_policy': 'Lossguide',\n",
    " 'task_type': 'CPU',\n",
    " 'subsample': 0.9094234167344891,\n",
    " 'random_state': 42,\n",
    " 'early_stopping_rounds': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839463ba-d160-44dc-9d69-97713309d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    " 'num_iterations': 3000,\n",
    " 'eval_metric': 'Quantile',\n",
    " 'n_jobs': -1,\n",
    " 'verbose': -1,\n",
    " 'max_depth': 10,\n",
    " 'num_leaves': 932,\n",
    " 'subsample_freq': 3,\n",
    " 'random_state': 42,\n",
    " 'n_estimators': 300,\n",
    " 'min_child_samples': 89,\n",
    " 'reg_alpha': 2.844326093512898,\n",
    " 'reg_lambda': 0.001222604907941908,\n",
    " 'subsample': 0.8528870880815199,\n",
    " 'learning_rate': 0.03170736097885979,\n",
    " 'colsample_bytree': 0.4884624716768982,\n",
    " 'min_child_weight': 46.817468629286644\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc04b5b-8877-4037-a1fa-14248ab6ec57",
   "metadata": {},
   "source": [
    "### 5.4.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f96d5-904f-417c-8ab7-a687f4ebff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def voting_cross_val(train, y, test, xgbr_params, lgbm_params, average=0):\n",
    "    \"\"\"\n",
    "    Do a K-Fold data cross-validation on a Voting Regressor model\n",
    "    \"\"\"\n",
    "    # Make a copy of an original training data set\n",
    "    train_fold = train.copy()\n",
    "    y_fold = y.copy()\n",
    "    \n",
    "    # Instantiate a numpy array filled with zeroes\n",
    "    results = np.zeros(len(test), dtype=np.float32)\n",
    "\n",
    "    # Instantiate a cross-validator\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Prepare data for cross-valdiation split\n",
    "    X = train_fold\n",
    "    y = y_fold\n",
    "\n",
    "    # Instantiate PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Fold', 'Validation RMSE', 'Validation RMSLE', 'Mean RMSLE']\n",
    "    \n",
    "    # Define best score (set to a high value as lower the score, the better)\n",
    "    best_score = 1e5\n",
    "\n",
    "    # Count folds\n",
    "    c = 1\n",
    "\n",
    "    # Fit the model with every cross-validation split\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        print(f\"{c} fold\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        x_val, y_val = X[val_idx], y[val_idx]\n",
    "        # Instantiate models\n",
    "        xgb_model = xgb.XGBRegressor(**xgbr_params)\n",
    "        cat_model = CatBoostRegressor(**cat_params)\n",
    "        lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "        # Instantiate voting model\n",
    "        voting_model = VotingRegressor(estimators=[('XGB', xgb_model), ('CAT', cat_model), ('LGBM', lgbm_model)], n_jobs=-1)\n",
    "        # Fit voting model\n",
    "        voting_model.fit(X_train, y_train)\n",
    "        y_preds = voting_model.predict(x_val)\n",
    "        predictions = voting_model.predict(test)\n",
    "        results = np.add(results, predictions)\n",
    "        rmsle = RMSLE(y_val, y_preds)\n",
    "        average += rmsle\n",
    "        table.add_row([c, voting_model.score(x_val, y_val), rmsle, average/c])\n",
    "        if best_score > rmsle:\n",
    "            best_score = rmsle\n",
    "        c += 1\n",
    "    print(table)\n",
    "    print(f\"Best RMSLE score: {best_score}\")\n",
    "    return best_score, results/(c-1)\n",
    "\n",
    "best_score, predictions = voting_cross_val(train, y, test, xgbr_params, lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bbff5-f415-485f-b3d5-3d61572e41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "xgb_model = xgb.XGBRegressor(**xgbr_params)\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "# Instantiate voting model\n",
    "voting_model = VotingRegressor(estimators=[('XGB', xgb_model), ('CAT', cat_model), ('LGBM', lgbm_model)], n_jobs=-1)\n",
    "# Fit voting model\n",
    "voting_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd8f51-c51b-45ef-868e-d21e550048e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "submission[\"Rings\"] = predictions\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1f333-6151-470b-8fc3-b61460aba8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
